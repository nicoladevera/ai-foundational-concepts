# LLMs 101

**Learning goal:** Understand how Large Language Models work, their capabilities and limitations, and why they behave the way they do.

---

# What is an LLM?

**LLM = Large Language Model**

A computer program trained on massive amounts of text to predict what words come next. That's it. Everything else builds on this core capability.

---

# LLMs as a Machine Learning Application

> ðŸ”— **Foundation Alert:** LLMs are built on Machine Learning principles. If you're new to ML concepts, start with [01 - Machine Learning 101](./01%20-%20Machine%20Learning%20101.md) to understand the fundamentals.

## How LLMs Fit in the ML Landscape

Large Language Models represent a specific application of **Deep Learning**, which itself is a subset of **Machine Learning**.

**The hierarchy:**
- **Machine Learning** = Teaching computers through examples
- **Deep Learning** = ML using neural networks with many layers
- **Transformers** = A specific deep learning architecture
- **LLMs** = Transformers trained on massive text datasets

## What Makes LLMs Different from Traditional ML

**Traditional ML (e.g., spam filter):**
- One model, one task
- Requires manual feature engineering
- Trained on structured data
- Thousands to millions of examples

**LLMs:**
- One model, hundreds of tasks
- Learns features automatically
- Trained on unstructured text
- Billions of examples
- Exhibits emergent abilities at scale

## The ML Techniques Behind LLMs

LLMs combine multiple ML approaches:

**Supervised Learning:** Predicting the next word in a sequence ("The cat sat on the ___")

**Unsupervised Learning:** Finding patterns in billions of text examples without explicit labels

**Transfer Learning:** One pre-trained model adapted to many different tasks

**Reinforcement Learning (RLHF):** Fine-tuning with human feedback to improve quality and safety

## Key ML Concepts Applied to LLMs

**Training Data:** Billions of web pages, books, code repositories
- Quality issue: Models learn biases present in training data
- Cutoff date: Models only know information from their training period

**Parameters:** The learned weights that determine model behavior
- GPT-4: ~1.7 trillion parameters
- GPT-5/GPT-5.1: Parameter count not publicly disclosed
- Claude Opus 4.5: Parameter count not publicly disclosed
- More parameters = more capacity but also more computing needed

**Inference:** What happens when you use ChatGPT or Claude
- The model uses its learned parameters to predict next tokens
- Relatively cheap compared to training

**Fine-Tuning:** Additional training on specialized data
- Example: Training Claude on code makes it better at programming
- Much cheaper than training from scratch

> ðŸ’¡ **Key Insight:** Everything LLMs do - understanding context, generating text, answering questions - comes from the same core ML task: predicting the next token based on patterns learned from billions of examples.

---

# How They Work (Simple Version)

## 1. Training Phase

The model reads billions of sentences from books, websites, code, etc. and learns patterns:
- "The cat sat on the ___" â†’ probably "mat" or "chair"
- "def calculate_sum(a, b):" â†’ probably "return a + b"

> ðŸ”‘ **Key Term:** Training = Teaching the model by showing it examples until it learns patterns

## 2. Tokens

LLMs don't read whole words - they break text into **tokens** (chunks of text, roughly 3-4 characters).

**Example:** "Hello world" â†’ ["Hello", " world"] (2 tokens)

> ðŸ’¡ **Why this matters:** Token limits (like "200,000 tokens") determine how much text the LLM can process at once. When you hit the limit, the LLM literally cannot "see" any more context - like trying to have a conversation while only remembering the last 10 sentences. This is why context engineering is so important.

## 3. Prediction = Generation

When you ask a question, the LLM:
1. Converts your text into tokens
2. Predicts the most likely next token
3. Adds that token to the response
4. Predicts the next token after that
5. Repeats until it decides to stop

> ðŸ’¡ **Key Insight:** LLMs don't "think" or "know" answers - they're incredibly sophisticated autocomplete.

---

# Key Concepts

## Context Window

The amount of text (measured in tokens) an LLM can "see" at once. Like short-term memory.

- **Analogy:** If you can only remember the last 10 sentences of a conversation, that's your context window
- **Example:** Claude Code has a 200,000 token context window (roughly 150,000 words)

> âš¡ **Why this is revolutionary:** Earlier LLMs had tiny context windows (4,000-8,000 tokens). Modern LLMs with 200,000+ token windows can "see" entire codebases, long documents, or complex conversations all at once - fundamentally changing what's possible.

## Parameters

The "brain size" of the model - how many connections it has learned.

- **GPT-4:** ~1.7 trillion parameters
- **Latest models (GPT-5.1, Claude Opus 4.5, Gemini 3 Pro):** Parameter counts not publicly disclosed
- **Bigger â‰  always better**, but generally means more nuanced understanding

> ðŸ“Š **Scale matters:** Parameter count roughly correlates with capability. Models with billions of parameters can do things smaller models can't - like reasoning about complex topics, writing coherent long-form content, or understanding subtle nuances. But they also cost more to run and require more computing power.

## Prompts

The input you give the LLM (your question, instructions, or context).

## Temperature

Controls randomness in responses:
- **Low (0.0-0.3):** Predictable, focused, factual
- **High (0.7-1.0):** Creative, varied, sometimes wild

## Fine-tuning

Additional training on specialized data to make the model better at specific tasks.

- **Example:** Training a base LLM on medical papers to create a medical AI assistant

## Embeddings

Converting text into numbers (vectors) that capture meaning.

- **Why useful:** Lets you mathematically compare similarity
- **Example:** "dog" and "puppy" have similar embeddings; "dog" and "banana" don't

---

# Important Limitations

## 1. No Real Understanding

LLMs are pattern matching, not reasoning. They can appear to understand but are predicting statistically likely responses.

## 2. Hallucinations

**Hallucination** = When the LLM confidently states false information

**Why it happens:** The model predicts plausible-sounding text, not necessarily true text.

> âš ï¸ **Critical to understand:** LLMs don't have a "truth" mechanism. They generate text that statistically fits the pattern, regardless of factual accuracy. They'll confidently cite non-existent research papers, make up statistics, or invent features that don't exist. Always verify important facts, especially for high-stakes decisions.

## 3. Knowledge Cutoff

LLMs only know information from their training data (e.g., Claude Opus 4.5's cutoff is March 2025, the most current among major models).

> ðŸ“… **Practical implication:** The LLM has never "seen" anything that happened after its training cutoff date. It can't tell you about recent events, new product releases, or current news unless you provide that information in your context. This is why RAG (Retrieval-Augmented Generation) and context engineering are so valuable - they bring current information into the conversation.

## 4. Context Matters

Same question with different context can produce very different answers.

---

# How Claude Code Uses LLMs Differently

## Standard chat LLM

You ask â†’ it responds with text

## Claude Code (agentic LLM)

- You ask
- It breaks down the task
- Uses tools (read files, run commands, search code)
- Iterates autonomously
- Responds with completed work

> ðŸ”§ **Key difference:** Tools extend what the LLM can do beyond just generating text. Instead of saying "here's code you should write," Claude Code can actually write the code, test it, see the errors, and fix them - all autonomously. This is the difference between an advisor and an assistant who can actually do the work.

---

# Analogy That Ties It Together

## LLM = A very well-read intern

- Read millions of documents (training)
- Can predict what usually comes next in conversations (generation)
- Has limited short-term memory (context window)
- Sometimes confidently wrong (hallucinations)
- Doesn't actually understand, just pattern-matches very well

## Claude Code = That intern with a computer

- Can look up information when needed (tools)
- Can execute tasks autonomously (agentic behavior)
- Can verify their own work (running tests, checking output)

---

> ðŸŽ¯ **Key Takeaway**
>
> LLMs are sophisticated prediction engines built on machine learning:
> - **Core Function:** Predict the next token based on patterns from billions of text examples
> - **Not True Understanding:** Pattern matching at massive scale, not reasoning or comprehension
> - **ML Foundation:** Combine supervised learning, unsupervised learning, and deep learning (neural networks)
> - **Key Limitations:** Hallucinations, knowledge cutoffs, context window limits
> - **Power Through Tools:** Agentic LLMs (like Claude Code) extend capabilities beyond text generation
>
> Understanding how LLMs work helps you:
> - Set realistic expectations about what they can and can't do
> - Craft better prompts and provide better context
> - Recognize when you're getting hallucinated vs. reliable information
> - Use them effectively as powerful assistants, not infallible oracles
>
> LLMs don't "know" things - they predict statistically likely responses. But with the right context and prompts, that prediction becomes incredibly useful.
